---
name: refine-feature
description: This workflow refines a feature ticket by gathering context, analyzing attachments, clarifying ambiguities, generating SMART acceptance criteria and test scenarios. Uses batched role processing for efficiency.
---

# Input
- feature description

# Output
No file must be created for this workflow. All outputs should be returned in the response and any code changes should be committed to the appropriate feature branch in the repository.

# Step 1 - Scope Determination & Initial Snapshot
- Determine scope: feature enhancement, bug fix, or refinement
- Gather context about the requirement
- Compose a concise 2-5 sentence plain-text summary of the feature scope from the user's input — this will be saved as the feature description in the database
- **[NEW - Rec 1]** Call `mcp__aiconductor__get_workflow_snapshot` to view any prior work on this feature (for context efficiency)

# Step 2 - Attachment Analysis
- For each attachment analyze:
  - Excel files: Extract columns, list items, data patterns
  - Images/designs: Extract design elements, component structure using Figma tools if applicable
  - Documents: Extract objectives, business rules, requirements
- Summarize key information from all attachments

# Step 3 - Comprehensive Clarifications
**CRITICAL: Capture all context to improve refinement quality. These questions inform AC, test scenarios, and task breakdown.**

- Identify ambiguous or incomplete requirements from Steps 1-2
- Do NOT ask about information already visible in attachments
- Present specific, focused clarifying questions organized by category
- Wait for user answers before proceeding
- **Store each answer via `mcp__aiconductor__add_clarification`** with the user's response

## 3.1 - Business & Stakeholder Context
Ask the user to clarify:
1. **Target Users/Personas**: Who will use this feature? (e.g., "internal admins", "external customers", "specific user segment")
   - Example: "Is this feature for all users or a specific segment?"
2. **Problem Statement**: What specific problem does this solve? (as opposed to nice-to-have)
   - Example: "Is this solving a critical bottleneck or improving convenience?"
3. **Success Metrics**: How will we measure if this feature succeeds?
   - Example: "Should we track adoption rate, performance improvement, or user satisfaction?"
4. **Timeline/Deadline**: When does this need to be completed?
   - Example: "Is this a sprint commitment, quarterly goal, or open-ended?"
5. **Priority Level**: Rate importance (Critical/High/Medium/Low)
   - Example: "Does this block other work or is it independent?"

## 3.2 - Technical & Integration Context
Ask the user to clarify:
6. **System Integration Points**: What other systems, APIs, or services does this integrate with?
   - Example: "Does this talk to any external APIs, databases, or services?"
7. **Performance Requirements**: Are there specific performance/scale requirements?
   - Example: "Expected throughput? Latency targets? Concurrent user load?"
8. **Data Volume & Scale**: How much data will this handle?
   - Example: "Millions of records? Real-time processing? Peak load times?"
9. **Platform/Browser Compatibility**: What platforms must this support?
   - Example: "All modern browsers? Mobile? Legacy browser support needed?"
10. **Backward Compatibility**: Must this work with existing systems without breaking changes?
    - Example: "Can we change the API/schema or must we maintain compatibility?"

## 3.3 - Risk, Compliance & Security Context
Ask the user to clarify:
11. **Security Requirements**: Are there specific security controls needed?
    - Example: "Authentication required? Data encryption? Role-based access control?"
12. **Compliance & Regulatory**: Are there compliance requirements (GDPR, HIPAA, SOC2)?
    - Example: "Subject to any regulatory requirements? Audit logging needed?"
13. **Data Privacy**: What data is handled and what privacy protections needed?
    - Example: "PII involved? Retention requirements? Data deletion needed?"
14. **Risk Assessment**: Are there known risks or failure scenarios to plan for?
    - Example: "What's the worst that could happen if this fails? Fallback needed?"

## 3.4 - Dependencies & Resources
Ask the user to clarify:
15. **Feature Dependencies**: Does this depend on other features/work?
    - Example: "Blocked by other tickets? Requires completion of earlier work?"
16. **Third-Party Services/Libraries**: Are external services or libraries required?
    - Example: "Payment gateway? Mapping service? Analytics provider?"
17. **Resource Constraints**: Are there limitations on resources, budget, or team?
    - Example: "Team size? Budget limits? Skills available (AI, ML, etc.)?"
18. **Documentation/Rollout**: What documentation or training is needed?
    - Example: "User guides? Admin documentation? Training required?"

## 3.5 - Edge Cases & Assumptions
Ask the user to clarify:
19. **Happy Path vs. Error Cases**: What should happen in error scenarios?
    - Example: "If API fails, should we retry? Show error? Use cached data?"
20. **Assumptions to Validate**: Document any assumptions made in Steps 1-2
    - Example: "Assuming X...is this correct? Should we adjust?"

---

**For each clarification question:**
- Call `mcp__aiconductor__add_clarification` with:
  - `question`: The clarification question text
  - `answer`: User's response (will be updated after user input)
  - `askedBy`: "llm"
- Use the AskUserQuestion tool to present 3-5 key clarifications per batch (don't overwhelm)
- Mark required clarifications vs. nice-to-have
- **After collecting all answers**, use them to inform Steps 4-6

# Step 4 - Acceptance Criteria Generation (Informed by Clarifications)
**Use the user's clarification answers from Step 3 to generate targeted acceptance criteria.**

- Create 5-8 SMART acceptance criteria (increased from 3-5 for better coverage):
  - Specific: No vague language
  - Measurable: Quantifiable outcomes (use metrics from clarifications Q3)
  - Achievable: Technically feasible (consider constraints from Q17)
  - Relevant: Tied to problem statement (use clarifications Q2)
  - Testable: Can be verified with a test
- **Cover:**
  - Happy path per target users (use Q1 personas)
  - Edge cases and error scenarios (use Q19 assumptions)
  - Performance/scale requirements (use Q7-Q8)
  - Security/compliance (use Q11-Q13)
  - Integration points (use Q6)
  - Data handling and privacy (use Q13)
  - Backward compatibility if needed (use Q10)
- Write each criterion as a clear, complete sentence in plain English
- Call `mcp__aiconductor__add_feature_acceptance_criteria` with the generated criteria

# Step 5 - Test Scenarios Generation (Informed by Clarifications)
**Use the user's clarification answers from Step 3 and AC from Step 4 to generate comprehensive test scenarios.**

- Create test scenarios with 1:1+ mapping to acceptance criteria
- **Test coverage based on clarifications:**
  - Happy path for each persona/user type (use Q1)
  - Error scenarios and recovery (use Q19)
  - Integration tests with external systems (use Q6)
  - Performance/load tests if applicable (use Q7-Q8)
  - Security/permission tests (use Q12)
  - Data handling tests (use Q13)
  - Backward compatibility tests if needed (use Q10)
  - Fallback/resilience scenarios (use Q14)
- For each scenario: clear preconditions and expected results as complete sentences
- Include:
  - Happy path scenarios
  - Edge cases and error conditions
  - Data boundary conditions
  - Concurrency/race condition scenarios (if relevant per Q8)
  - Security validation scenarios (if relevant per Q11-Q12)
- Mark scenarios as automated vs. manual-only (use Q17 resource constraints)
- Ensure all scenarios are specific and repeatable
- Call `mcp__aiconductor__add_feature_test_scenarios` with the generated scenarios

# Step 6 - Task Breakdown and Generation (Informed by Clarifications)
**Use clarifications from Step 3, AC from Step 4, and test scenarios from Step 5 to design task breakdown.**

- Break the feature into 5-8 discrete, actionable tasks
- **Consider clarifications when designing tasks:**
  - Prioritize based on timeline (Q4) and dependencies (Q15)
  - Design for target personas/users (Q1)
  - Allocate tasks based on team skills (Q17)
  - Create separate tasks for security/compliance if needed (Q11-Q13)
  - Plan integration work separately (Q6)
- For each task:
  - Assign unique task identifier (T01, T02, etc.)
  - Set initial status to "PendingProductDirector"
  - Define clear task title and description
  - **Link to clarifications**: Reference the relevant user answers in task descriptions
  - Map relevant acceptance criteria to the task
  - Map relevant test scenarios to the task
  - Define what is out of scope for this task
  - Set orderOfExecution (sequential numbering)
  - Estimate effort considering team skills and complexity (use Q17)
- Use the MCP tool `mcp__aiconductor__create_feature` to create the feature entry
  - **Pass the `description` parameter** with the 2-5 sentence plain-text summary composed in Step 1
  - The description must be the original user requirement text (condensed) — NOT a hallucinated value
  - Example: `{ featureSlug: "...", featureName: "...", description: "...", repoName: "..." }`
- Use the MCP tool `mcp__aiconductor__add_task` for each task
- Ensure each task is:
  - Independently testable
  - Has clear boundaries
  - Includes all necessary acceptance criteria
  - Maps to specific test scenarios

# Step 6.5 - Validate Task Dependencies & Execution Order [NEW - Rec 4]
- Call `mcp__aiconductor__get_task_execution_plan` to analyze dependencies
- Review optimal order, parallelizable phases, and any circular dependencies or warnings
- If dependencies need adjustment, update tasks using `mcp__aiconductor__update_task`

# Step 7 - Stakeholder Review Cycle (Batched by Role)
**CRITICAL: Process ALL tasks through each role in a single batch before moving to the next role. This is far more efficient than switching roles per task.**

## 7.0 - Initialize Task List
- **[UPDATED - Rec 1]** Call `mcp__aiconductor__get_workflow_snapshot` for context-efficient overview (instead of `get_tasks_by_status`)
- Store the task IDs for reference

## 7.1 - PRODUCT DIRECTOR REVIEW (BATCH ALL TASKS)
**Single role adoption for entire batch:**
- Call `mcp__aiconductor__get_next_step` to get the systemPrompt for Product Director role
- Adopt Product Director identity ONCE for this entire batch
- Get all tasks with status "PendingProductDirector"
- For each task (process ALL together):
  - Review task with Product Director perspective
  - Evaluate market demand and user value
  - Identify any product concerns
  - Prepare detailed review notes
- Conduct required research once for the batch (market analysis, competitor analysis)
- For each task:
  - **[NEW - Rec 7]** Call `mcp__aiconductor__validate_review_completeness` before submitting to check all required fields are present
  - Call `mcp__aiconductor__add_stakeholder_review` with:
    - decision: "approve" or "reject"
    - notes: Detailed review for this specific task
    - additionalFields: `marketAnalysis`, `competitorAnalysis`, **`quickSummary`** (1-2 sentence TL;DR - Rec 6)
- All approved tasks auto-transition to "PendingArchitect"
- Rejected tasks auto-transition to "NeedsRefinement"
- **Progress output**: "Product Director batch complete: [N] approved, [M] rejected"

## 7.2 - ARCHITECT REVIEW (BATCH ALL TASKS)
**Single role adoption for entire batch:**
- Call `mcp__aiconductor__get_next_step` to get the systemPrompt for Architect role
- Adopt Architect identity ONCE for this entire batch
- Get all tasks with status "PendingArchitect"
- For each task (process ALL together):
  - Review task with Architect perspective
  - Evaluate technical approach and design patterns
  - Review Product Director notes from previousRoleNotes
  - Identify any technical concerns or improvements
  - Prepare detailed review notes
- Conduct required research once for the batch (design patterns, best practices, technologies)
- For each task: Call `mcp__aiconductor__add_stakeholder_review` with:
  - decision: "approve" or "reject"
  - notes: Detailed review for this specific task
  - additionalFields: `technologyRecommendations`, `designPatterns`
- All approved tasks auto-transition to "PendingUiUxExpert"
- Rejected tasks auto-transition to "NeedsRefinement"
- **Progress output**: "Architect batch complete: [N] approved, [M] rejected"

## 7.3 - UI/UX EXPERT REVIEW (BATCH ALL TASKS)
**Single role adoption for entire batch:**
- Call `mcp__aiconductor__get_next_step` to get the systemPrompt for UI/UX Expert role
- Adopt UI/UX Expert identity ONCE for this entire batch
- Get all tasks with status "PendingUiUxExpert"
- For each task (process ALL together):
  - Review task with UI/UX Expert perspective
  - Evaluate usability, accessibility, and user experience
  - Review Product Director and Architect notes from previousRoleNotes
  - Identify any UX concerns or improvements
  - Prepare detailed review notes
- Conduct required research once for the batch (UX best practices, accessibility, user behavior)
- For each task: Call `mcp__aiconductor__add_stakeholder_review` with:
  - decision: "approve" or "reject"
  - notes: Detailed review for this specific task
  - additionalFields: `usabilityFindings`, `accessibilityRequirements`, `userBehaviorInsights`
- All approved tasks auto-transition to "PendingSecurityOfficer"
- Rejected tasks auto-transition to "NeedsRefinement"
- **Progress output**: "UI/UX Expert batch complete: [N] approved, [M] rejected"

## 7.4 - SECURITY OFFICER REVIEW (BATCH ALL TASKS)
**Single role adoption for entire batch:**
- Call `mcp__aiconductor__get_next_step` to get the systemPrompt for Security Officer role
- Adopt Security Officer identity ONCE for this entire batch
- Get all tasks with status "PendingSecurityOfficer"
- For each task (process ALL together):
  - Review task with Security Officer perspective
  - Evaluate security requirements and compliance
  - Review all previous stakeholder notes from previousRoleNotes
  - Identify any security concerns or improvements
  - Prepare detailed review notes
- Conduct required research once for the batch (OWASP guidelines, security best practices, compliance)
- For each task: Call `mcp__aiconductor__add_stakeholder_review` with:
  - decision: "approve" or "reject"
  - notes: Detailed review for this specific task
  - additionalFields: `securityRequirements`, `complianceNotes`
- All approved tasks auto-transition to "ReadyForDevelopment"
- Rejected tasks auto-transition to "NeedsRefinement"
- **Progress output**: "Security Officer batch complete: [N] approved, [M] rejected"

## 7.5 - Handle Rejected Tasks (If Any)
- Call `mcp__aiconductor__get_tasks_by_status` with status "NeedsRefinement" to find rejected tasks
- If any tasks were rejected:
  - Review the issues identified by each stakeholder
  - Update task details using `mcp__aiconductor__update_task` to address concerns
  - **[NEW - Rec 3]** Optionally: Use `mcp__aiconductor__rollback_last_decision` to undo incorrect review (if needed)
  - Call `mcp__aiconductor__transition_task_status` to move back to "PendingProductDirector"
  - Restart the review cycle from Step 7.1 for these tasks
  - Repeat until all tasks reach "ReadyForDevelopment"

## 7.6 - Verification
- Call `mcp__aiconductor__get_tasks_by_status` with status "ReadyForDevelopment"
- Confirm all tasks created in Step 6 are now in "ReadyForDevelopment" status
- If verification fails, report which tasks are stuck and why

# Step 8 - Finalization & Checkpoint
- Combine all acceptance criteria into a single text block
- Combine all test scenarios into a single text block
- If using Jira integration: Update the Jira ticket with final AC and test scenarios
- **[NEW - Rec 3]** Call `mcp__aiconductor__save_workflow_checkpoint` with description "All tasks ReadyForDevelopment - ready for dev workflow"
- **[NEW]** If the feature description has been refined or improved during the workflow (Steps 3-5 may have clarified the scope), call `mcp__aiconductor__update_feature` with the final polished description so the dashboard Detail view shows accurate context
- Present the final AC and test scenarios to the user
- Confirm workflow completion

